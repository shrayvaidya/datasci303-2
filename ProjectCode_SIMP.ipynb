{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Project Code\"\n",
    "subtitle: SIMP\n",
    "author: Mingyi Gong, Peggy Han, Izzy Podolsky, Shray Vaidya\n",
    "date: 02/27/2023\n",
    "number-sections: true\n",
    "abstract: _This file contains the code for the project on college ranking, as part of the STAT303-2 course in Winter 2023_.\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "## Data quality check / cleaning / preparation \n",
    "\n",
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.** An example is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140c19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db149d8b",
   "metadata": {},
   "source": [
    "### Data quality check\n",
    "*By Elton John*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc82e7f",
   "metadata": {},
   "source": [
    "The code below visualizes the distribution of all the variables in the dataset, and their association with the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5955618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Distribution of continuous variables...#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8faafdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Distribution of categorical variables...#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e389f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Association of the response with the predictors...#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1561829",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "*By Peggy Han*\n",
    "\n",
    "For data cleaning, we performed the following:\n",
    "\n",
    "1. Some of the columns only have NaN values, so we removed all these variables to make the data set simpler. We also dropped some irrelevant variables that wouldn't help with developing the model.\n",
    "\n",
    "2. In the second dataset from obtained from College Scorecard, there are about 3000 different variables. We manually selected some variables that might be relevant to be response based on the variable description.\n",
    "\n",
    "3. When merging the two data sets, we found that some of the rows are duplicated in the resulting data frame because more than one insitution in the second dataset has the same school code, and some schools have different codes in the two data sets, so they have missing columns from the second data set. We did some manual deletion and filling in of the merged data.\n",
    "\n",
    "4. We identified that some of the institutions were unranked in the Kaggle data set, so we dropped those rows. We also dropped columns with only 1 unique value as they do not provide insight for building the model.\n",
    "\n",
    "5. Some of the variables from the Collge Scorecard data contains the same information are separately stored into private and public columns. For private schools, the information is stored in the private column and the public column is NaN, and vice versa. We combined these columns to use the variable as a predictor with no NaN values in the column.\n",
    "\n",
    "6. Both data sets contain the information for average SAT score, enrollment, and admission rate information. We think the data from College Scorecard is more accurate, so we prioritize the College Scorecard data and use the kaggle data to fill in some of the missing values in the College Scorecard column to minimize the number of missing values in the variables.\n",
    "\n",
    "The code below implements the above cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f73626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('project_data/schoolInfo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a236706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped columns which only have NaN values\n",
    "df1.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Dropped irrelevant variables\n",
    "df1 = df1.drop(['nonResponderText', 'nonResponder', 'primaryPhoto', 'primaryPhotoThumb', 'aliasNames', 'urlName'], \n",
    "         axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd5c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/7c21x7qj4fv7flh8p9m2wwn00000gn/T/ipykernel_86468/539635706.py:1: DtypeWarning: Columns (1729,1909,1910,1911,1912,1913) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('project_data/MERGED2017_18_PP.csv')\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('project_data/MERGED2017_18_PP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5c70f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/7c21x7qj4fv7flh8p9m2wwn00000gn/T/ipykernel_86468/122189018.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_slice.dropna(axis=1, how='all', inplace=True)\n",
      "/var/folders/vk/7c21x7qj4fv7flh8p9m2wwn00000gn/T/ipykernel_86468/122189018.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_slice['primaryKey'] = df2_slice['OPEID6']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(420, 78)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually selected some variables that seem relevant based on description\n",
    "df2_slice = df2[['OPEID6','INSTNM','SCH_DEG','NUMBRANCH','PREDDEG','HIGHDEG','REGION','ADM_RATE','SATVR25','SATVR75','SATMT25',\n",
    "                'SATMT75','SATWR25','SATWR75','SATVRMID','SATMTMID','SATWRMID','ACTCM25','ACTCM75','ACTEN25',\n",
    "                'ACTEN75','ACTMT25','ACTMT75','ACTWR25','ACTWR75','ACTCMMID','ACTENMID','ACTMTMID','ACTWRMID',\n",
    "                'SAT_AVG','UGDS','UGDS_WHITE','UGDS_BLACK','UGDS_HISP','UGDS_ASIAN','UGDS_AIAN','UGDS_NHPI',\n",
    "                'UGDS_2MOR','UGDS_NRA','UGDS_UNKN','PPTUG_EF','NPT4_PUB','NPT4_PRIV','NUM4_PUB','NUM4_PRIV',\n",
    "                'NUM4_PROG','NUM4_OTHER','COSTT4_A','COSTT4_P','TUITIONFEE_IN','TUITIONFEE_OUT','TUITIONFEE_PROG',\n",
    "                'TUITFTE','INEXPFTE','AVGFACSAL','PFTFAC']]\n",
    "\n",
    "# Dropped columns with only NA\n",
    "df2_slice.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Use the school code to create a matching column to merge the two datasets\n",
    "df2_slice['primaryKey'] = df2_slice['OPEID6']\n",
    "\n",
    "result = pd.merge(df1, df2_slice, on='primaryKey', how=\"left\", indicator = True)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef95aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 78)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identified duplicated rows\n",
    "duplicated = result[result['primaryKey'].duplicated(keep=False)]\n",
    "duplicated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1b3a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded data to perform manual selection\n",
    "#result.to_csv('processed_data_1.csv', index=False)\n",
    "#duplicated.to_csv('duplicated_1.csv', index=False)\n",
    "\n",
    "# Read the manually processed data\n",
    "data = pd.read_csv('project_data/processed_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae93d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act-avg</th>\n",
       "      <th>sat-avg</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>city</th>\n",
       "      <th>sortName</th>\n",
       "      <th>zip</th>\n",
       "      <th>acceptance-rate</th>\n",
       "      <th>rankingDisplayScore</th>\n",
       "      <th>percent-receiving-aid</th>\n",
       "      <th>cost-after-aid</th>\n",
       "      <th>...</th>\n",
       "      <th>COSTT4_A</th>\n",
       "      <th>COSTT4_P</th>\n",
       "      <th>TUITIONFEE_IN</th>\n",
       "      <th>TUITIONFEE_OUT</th>\n",
       "      <th>TUITIONFEE_PROG</th>\n",
       "      <th>TUITFTE</th>\n",
       "      <th>INEXPFTE</th>\n",
       "      <th>AVGFACSAL</th>\n",
       "      <th>PFTFAC</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>alliantinternationaluniversity</td>\n",
       "      <td>92131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orange</td>\n",
       "      <td>argosyuniversity</td>\n",
       "      <td>92868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>29396.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13438.0</td>\n",
       "      <td>13438.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16793.0</td>\n",
       "      <td>6133.0</td>\n",
       "      <td>5460.0</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>californiainstituteofintegralstudies</td>\n",
       "      <td>94103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22916.0</td>\n",
       "      <td>13260.0</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>0.3247</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>capellauniversity</td>\n",
       "      <td>55403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19836.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14250.0</td>\n",
       "      <td>14250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16533.0</td>\n",
       "      <td>1714.0</td>\n",
       "      <td>6841.0</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pocatello</td>\n",
       "      <td>idahostateuniversity</td>\n",
       "      <td>83209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19592.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7166.0</td>\n",
       "      <td>21942.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7822.0</td>\n",
       "      <td>11741.0</td>\n",
       "      <td>7156.0</td>\n",
       "      <td>0.9380</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.0</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>northcentraluniversity</td>\n",
       "      <td>86314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16529.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>6347.0</td>\n",
       "      <td>0.2204</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cypress</td>\n",
       "      <td>tridentuniversityinternational</td>\n",
       "      <td>90630</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17544.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9240.0</td>\n",
       "      <td>9240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9361.0</td>\n",
       "      <td>1828.0</td>\n",
       "      <td>5705.0</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>unioninstituteanduniversity</td>\n",
       "      <td>45206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24696.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12896.0</td>\n",
       "      <td>12896.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16910.0</td>\n",
       "      <td>6638.0</td>\n",
       "      <td>5540.0</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>universityofphoenix</td>\n",
       "      <td>85034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20083.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9608.0</td>\n",
       "      <td>9608.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13180.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>waldenuniversity</td>\n",
       "      <td>55401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12465.0</td>\n",
       "      <td>12465.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10183.0</td>\n",
       "      <td>2854.0</td>\n",
       "      <td>6769.0</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8873.0</td>\n",
       "      <td>New Castle</td>\n",
       "      <td>wilmingtonuniversity</td>\n",
       "      <td>19720</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17529.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10940.0</td>\n",
       "      <td>10940.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10203.0</td>\n",
       "      <td>3833.0</td>\n",
       "      <td>7197.0</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     act-avg  sat-avg  enrollment           city  \\\n",
       "300      NaN      NaN      1264.0      San Diego   \n",
       "301      NaN      NaN         NaN         Orange   \n",
       "302      NaN      NaN         NaN  San Francisco   \n",
       "303      NaN      NaN         NaN    Minneapolis   \n",
       "304      NaN      NaN         NaN      Pocatello   \n",
       "305      NaN      NaN       133.0      San Diego   \n",
       "306      NaN      NaN         NaN        Cypress   \n",
       "307      NaN      NaN         NaN     Cincinnati   \n",
       "308      NaN      NaN         NaN        Phoenix   \n",
       "309      NaN      NaN         NaN    Minneapolis   \n",
       "310      NaN      NaN      8873.0     New Castle   \n",
       "\n",
       "                                 sortName    zip  acceptance-rate  \\\n",
       "300        alliantinternationaluniversity  92131              NaN   \n",
       "301                      argosyuniversity  92868              NaN   \n",
       "302  californiainstituteofintegralstudies  94103              NaN   \n",
       "303                     capellauniversity  55403              NaN   \n",
       "304                  idahostateuniversity  83209              NaN   \n",
       "305                northcentraluniversity  86314              NaN   \n",
       "306        tridentuniversityinternational  90630             96.0   \n",
       "307           unioninstituteanduniversity  45206              NaN   \n",
       "308                   universityofphoenix  85034              NaN   \n",
       "309                      waldenuniversity  55401              NaN   \n",
       "310                  wilmingtonuniversity  19720            100.0   \n",
       "\n",
       "     rankingDisplayScore  percent-receiving-aid  cost-after-aid  ... COSTT4_A  \\\n",
       "300                  NaN                    NaN             NaN  ...      NaN   \n",
       "301                  NaN                    NaN             NaN  ...  29396.0   \n",
       "302                  NaN                    NaN             NaN  ...      NaN   \n",
       "303                  NaN                    NaN             NaN  ...  19836.0   \n",
       "304                  NaN                    NaN             NaN  ...  19592.0   \n",
       "305                  NaN                    NaN             NaN  ...      NaN   \n",
       "306                  NaN                    NaN             NaN  ...  17544.0   \n",
       "307                  NaN                    NaN             NaN  ...  24696.0   \n",
       "308                  NaN                    NaN             NaN  ...  20083.0   \n",
       "309                  NaN                    NaN             NaN  ...      NaN   \n",
       "310                  NaN                    NaN             NaN  ...  17529.0   \n",
       "\n",
       "     COSTT4_P  TUITIONFEE_IN TUITIONFEE_OUT TUITIONFEE_PROG  TUITFTE  \\\n",
       "300       NaN            NaN            NaN             NaN      NaN   \n",
       "301       NaN        13438.0        13438.0             NaN  16793.0   \n",
       "302       NaN            NaN            NaN             NaN  22916.0   \n",
       "303       NaN        14250.0        14250.0             NaN  16533.0   \n",
       "304       NaN         7166.0        21942.0             NaN   7822.0   \n",
       "305       NaN            NaN            NaN             NaN  16529.0   \n",
       "306       NaN         9240.0         9240.0             NaN   9361.0   \n",
       "307       NaN        12896.0        12896.0             NaN  16910.0   \n",
       "308       NaN         9608.0         9608.0             NaN  13180.0   \n",
       "309       NaN        12465.0        12465.0             NaN  10183.0   \n",
       "310       NaN        10940.0        10940.0             NaN  10203.0   \n",
       "\n",
       "     INEXPFTE  AVGFACSAL  PFTFAC     _merge  \n",
       "300       NaN        NaN     NaN  left_only  \n",
       "301    6133.0     5460.0  0.1774       both  \n",
       "302   13260.0     8190.0  0.3247       both  \n",
       "303    1714.0     6841.0  0.1387       both  \n",
       "304   11741.0     7156.0  0.9380       both  \n",
       "305    2909.0     6347.0  0.2204  left_only  \n",
       "306    1828.0     5705.0  0.0614       both  \n",
       "307    6638.0     5540.0  0.1690       both  \n",
       "308    2042.0     4485.0  0.0462  left_only  \n",
       "309    2854.0     6769.0  0.0674       both  \n",
       "310    3833.0     7197.0  0.0497       both  \n",
       "\n",
       "[11 rows x 78 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identified unranked institutions\n",
    "data.loc[data.rankingDisplayRank == \"Unranked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f771527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed unranked institutions\n",
    "data.drop(index = range(300,311), inplace = True)\n",
    "\n",
    "# Dropped columns with only NA\n",
    "data.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Drop columns with only 1 unique value\n",
    "cols_to_drop = []\n",
    "for col in data.columns:\n",
    "    if data[col].nunique() == 1:\n",
    "        cols_to_drop.append(col)\n",
    "data.drop(cols_to_drop, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b51a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined columns that have the same information but stored separately for public and private institutions\n",
    "data['NPT4'] = data['NPT4_PUB'].fillna(data['NPT4_PRIV'])\n",
    "data['NUM4'] = data['NUM4_PUB'].fillna(data['NUM4_PRIV'])\n",
    "\n",
    "# Dropped already combined columns and _merge\n",
    "data.drop(['_merge','NPT4_PUB','NPT4_PRIV','NUM4_PUB','NUM4_PRIV'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5506da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new colume that contains SAT Average \n",
    "# Filled missing values in College Scorecard SAT average with values of SAT average from the Kaggle data \n",
    "# to minimize number of missing values\n",
    "data['sat_avg'] = data['SAT_AVG'].fillna(data['sat-avg'])\n",
    "\n",
    "# Dropped the two original columns\n",
    "data.drop(['SAT_AVG','sat-avg'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1940c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the same principle, we will use enrollment data from college scorecard instead of Kaggle\n",
    "# Dropped Kaggle enrollment data\n",
    "data.drop(['enrollment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af58ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variables to more interpretable names\n",
    "data = data.rename(columns = {\n",
    "    'act-avg': 'act_avg',\n",
    "    'acceptance-rate': 'acceptance_rate',\n",
    "    'percent-receiving-aid': 'percent_receiving_aid',\n",
    "    'cost-after-aid': 'cost_after_aid',\n",
    "    'hs-gpa-avg': 'hs_gpa_avg', \n",
    "    'INSTNM': 'institution_name',\n",
    "    'NUMBRANCH': 'branches', \n",
    "    'REGION': 'region',\n",
    "    'ADM_RATE': 'admission_rate',\n",
    "    'SATVR25': 'satCR25', \n",
    "    'SATVR75': 'satCR75',\n",
    "    'SATMT25': 'satmt25',\n",
    "    'SATMT75': 'satmt75',\n",
    "    'SATVRMID': 'satcrmid', \n",
    "    'SATMTMID': 'satmtmid',\n",
    "    'ACTCM25': 'actcm25',\n",
    "    'ACTCM75': 'actcm75',\n",
    "    'ACTEN25': 'acten25', \n",
    "    'ACTEN75': 'acten75',\n",
    "    'ACTMT25': 'actmt25',\n",
    "    'ACTMT75': 'actmt75',\n",
    "    'ACTCMMID': 'actcmmid', \n",
    "    'ACTENMID': 'actenmid',\n",
    "    'ACTMTMID': 'actmtmid',\n",
    "    'UGDS': 'enrollment', \n",
    "    'UGDS_WHITE': 'percent_white',\n",
    "    'UGDS_BLACK': 'percent_black',\n",
    "    'UGDS_HISP': 'percent_hispanic',\n",
    "    'UGDS_ASIAN': 'percent_asian', \n",
    "    'UGDS_AIAN': 'percent_aian',\n",
    "    'UGDS_NHPI': 'percent_nhpi', \n",
    "    'UGDS_2MOR': 'percent_twoormore',\n",
    "    'UGDS_NRA': 'percent_nra',\n",
    "    'UGDS_UNKN': 'percent_unknown', \n",
    "    'PPTUG_EF': 'percent_parttime',\n",
    "    'COSTT4_A': 'avg_cost',\n",
    "    'TUITIONFEE_IN': 'instante_tuition', \n",
    "    'TUITIONFEE_OUT': 'outstate_tuition',\n",
    "    'TUITFTE': 'tuition_revenue_per', \n",
    "    'INEXPFTE': 'instructional_expenditure_per', \n",
    "    'AVGFACSAL': 'avg_faculty_salary', \n",
    "    'PFTFAC': 'ft_faculty_rate', \n",
    "    'NPT4': 'avg_net_price', \n",
    "    'NUM4': 'number_titleIV'\n",
    "}\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e11d4042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_avg</th>\n",
       "      <th>city</th>\n",
       "      <th>sortName</th>\n",
       "      <th>zip</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>rankingDisplayScore</th>\n",
       "      <th>percent_receiving_aid</th>\n",
       "      <th>cost_after_aid</th>\n",
       "      <th>state</th>\n",
       "      <th>rankingSortRank</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_cost</th>\n",
       "      <th>instante_tuition</th>\n",
       "      <th>outstate_tuition</th>\n",
       "      <th>tuition_revenue_per</th>\n",
       "      <th>instructional_expenditure_per</th>\n",
       "      <th>avg_faculty_salary</th>\n",
       "      <th>ft_faculty_rate</th>\n",
       "      <th>avg_net_price</th>\n",
       "      <th>number_titleIV</th>\n",
       "      <th>sat_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>16.0</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>tennesseestateuniversity</td>\n",
       "      <td>37209</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TN</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>19058.0</td>\n",
       "      <td>7776.0</td>\n",
       "      <td>21132.0</td>\n",
       "      <td>6877.0</td>\n",
       "      <td>8732.0</td>\n",
       "      <td>7310.0</td>\n",
       "      <td>0.9707</td>\n",
       "      <td>11083.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>788.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     act_avg       city                  sortName    zip  acceptance_rate  \\\n",
       "263     16.0  Nashville  tennesseestateuniversity  37209             53.0   \n",
       "\n",
       "     rankingDisplayScore  percent_receiving_aid  cost_after_aid state  \\\n",
       "263                  NaN                    NaN             NaN    TN   \n",
       "\n",
       "     rankingSortRank  ...  avg_cost instante_tuition  outstate_tuition  \\\n",
       "263               -1  ...   19058.0           7776.0           21132.0   \n",
       "\n",
       "     tuition_revenue_per  instructional_expenditure_per  avg_faculty_salary  \\\n",
       "263               6877.0                         8732.0              7310.0   \n",
       "\n",
       "     ft_faculty_rate avg_net_price  number_titleIV sat_avg  \n",
       "263           0.9707       11083.0           609.0   788.0  \n",
       "\n",
       "[1 rows x 63 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identified missing value in admission_rate\n",
    "missing = data['admission_rate'].isna()\n",
    "na_rows = data[missing]\n",
    "na_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f54ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filled in missing value in College Scorecard admission rate data with Kaggle acceptance rate data\n",
    "ar = data.loc[263, 'acceptance_rate']/100\n",
    "data.at[263, 'admission_rate'] = ar\n",
    "data.drop(['acceptance_rate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded cleaned data\n",
    "data.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91a14e",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "*By Sankaranarayanan Balasubramanian and Chun-Li*\n",
    "\n",
    "The following data preparation steps helped us to prepare our data for implementing various modeling / validation techniques:\n",
    "\n",
    "1. Since we need to predict house price, we derived some new predictors *(from existing predictors)* that intuitively seem to be helpuful to predict house price. \n",
    "\n",
    "2. We have shuffled the dataset to prepare it for K-fold cross validation.\n",
    "\n",
    "3. We have created a standardized version of the dataset, as we will use it to develop Lasso / Ridge regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b2b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######---------------Creating new predictors----------------#########\n",
    "\n",
    "#Creating number of bedrooms per unit floor area\n",
    "\n",
    "#Creating ratio of bathrooms to bedrooms\n",
    "\n",
    "#Creating ratio of carpet area to floor area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e04063",
   "metadata": {},
   "outputs": [],
   "source": [
    "######-----------Shuffling the dataset for K-fold------------#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######-----Standardizing the dataset for Lasso / Ridge-------#########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd74a9",
   "metadata": {},
   "source": [
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ec4c9",
   "metadata": {},
   "source": [
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229b491",
   "metadata": {},
   "source": [
    "# Developing main model for LOOCV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4867342",
   "metadata": {},
   "source": [
    "*By Mingyi Gong*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3da12c",
   "metadata": {},
   "source": [
    "We will first develop loocv to classify different observations to different categories. Many observations have invalid/NA rankings. We use two approaches for this problem: (1) we fill those ranking with mean (2) we drop those observations with invalid rankings. We will first run code on dataset from approach (1) and then approach (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7ed585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a51ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on dataset from data cleaning\n",
    "df = pd.read_csv('~/Desktop/303/newdata.csv')\n",
    "df= df.fillna(df.mean())\n",
    "df2 = df.loc[df.rankingSortRank > 0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead5318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting columns in int and float type for loocv\n",
    "X = df[['act_avg', 'sat_avg','percent_receiving_aid',\n",
    "       'cost_after_aid', 'hs_gpa_avg','businessRepScore', 'tuition',\n",
    "       'engineeringRepScore','branches', 'admission_rate', \n",
    "       'ug_enrollment', 'percent_white', 'percent_black', 'percent_hispanic',\n",
    "       'percent_asian', 'percent_aian', 'percent_nhpi', 'percent_twoormore',\n",
    "       'percent_nra', 'percent_unknown', 'percent_parttime', 'avg_cost',\n",
    "       'instante_tuition', 'outstate_tuition', 'tuition_revenue_per',\n",
    "       'instructional_expenditure_per', 'avg_faculty_salary',\n",
    "       'ft_faculty_rate', 'avg_net_price', 'number_titleIV']]\n",
    "Y = df[['rankingSortRank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756df188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 3, 5, 5, 5, 8, 9, 10, 11, 11, 11, 14, 14, 14, 14, 18, 18, 20, 21, 21, 21, 21, 25, 25, 27, 28, 29, 30, 30, 32, 32, 34, 34, 34, 37, 37, 37, 40, 40, 42, 42, 42, 42, 46, 46, 46, 46, 46, 46, 52, 52, 54, 54, 56, 56, 56, 56, 56, 61, 61, 61, 61, 61, 61, 67, 68, 69, 69, 69, 69, 69, 69, 75, 75, 75, 78, 78, 78, 81, 81, 81, 81, 81, 81, 87, 87, 87, 90, 90, 90, 90, 94, 94, 94, 97, 97, 97, 97, 97, 97, 103, 103, 103, 103, 103, 103, 103, 110, 110, 110, 110, 110, 115, 115, 115, 115, 115, 120, 120, 120, 120, 124, 124, 124, 124, 124, 124, 124, 124, 132, 133, 133, 133, 133, 133, 133, 133, 140, 140, 140, 140, 140, 145, 145, 145, 145, 145, 145, 151, 151, 151, 151, 151, 156, 156, 156, 159, 159, 159, 159, 159, 159, 165, 165, 165, 165, 165, 165, 171, 171, 171, 171, 171, 176, 176, 176, 176, 176, 181, 181, 181, 181, 181, 181, 187, 187, 187, 187, 187, 192, 192, 192, 192, 192, 192, 198, 198, 198, 198, 202, 202, 202, 202, 202, 207, 207, 207, 207, 207, 207, 207, 207, 207, 216, 216, 216, 216, 216, 216, 216, 223, 223, 223, 223, 223, 223, 223, 223, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "-----\n",
      "[3, 14, 11, 10, 5, 5, 5, 14, 11, 3, 14, 18, 21, 25, 56, 11, 14, 34, 3, 11, 11, 81, 21, 14, 20, 46, 11, 25, 46, 110, 28, 46, 67, 34, 133, 18, 30, 11, 124, 90, 46, 46, 42, 56, 81, 29, 90, 42, 40, 81, 69, 115, 115, 115, 81, 40, 21, 97, 81, 21, 30, 69, 87, 94, 81, 75, 103, 75, 90, 115, 223, 54, 56, 69, 120, 181, 140, 133, 61, 81, 34, 176, 56, 140, 165, 78, 171, 46, -1, 54, 103, 78, 61, 124, 37, 151, 61, 145, 181, 207, 133, 25, 133, 34, 69, 156, 124, -1, 124, 97, 140, 133, 32, 192, 54, 69, 52, 97, 159, 94, 133, 61, 140, 94, 133, 181, 103, 90, 42, 94, 140, 61, -1, 87, 181, 198, 171, 171, 145, 103, 46, -1, 61, 145, 207, 171, 181, 192, 97, 81, 61, 165, 171, 124, 171, -1, 103, 103, 151, 192, 207, 34, 124, 207, 176, 223, 207, 61, 34, 97, -1, 151, 69, 207, 124, -1, 181, -1, 165, 94, -1, 181, 181, 133, 103, 216, 124, -1, -1, 165, 198, -1, 56, 140, -1, 207, 124, -1, 207, 110, -1, -1, -1, 181, 81, 207, -1, 171, -1, 216, 207, 21, 159, -1, 198, -1, -1, -1, 97, 207, 207, -1, -1, -1, -1, -1, -1, 181, -1, 192, -1, 97, 176, -1, -1, 181, -1, 216, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 124, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 192, -1, -1, -1, -1, -1, -1, -1, -1, 202, -1, -1, -1, 223, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Accuracy: 0.24333333333333335\n"
     ]
    }
   ],
   "source": [
    "# use loocv to predict and get accuracy\n",
    "X = df[['act_avg', 'sat_avg','percent_receiving_aid',\n",
    "       'cost_after_aid', 'hs_gpa_avg','businessRepScore', 'tuition',\n",
    "       'engineeringRepScore','branches', 'admission_rate', \n",
    "       'ug_enrollment', 'percent_white', 'percent_black', 'percent_hispanic',\n",
    "       'percent_asian', 'percent_aian', 'percent_nhpi', 'percent_twoormore',\n",
    "       'percent_nra', 'percent_unknown', 'percent_parttime', 'avg_cost',\n",
    "       'instante_tuition', 'outstate_tuition', 'tuition_revenue_per',\n",
    "       'instructional_expenditure_per', 'avg_faculty_salary',\n",
    "       'ft_faculty_rate', 'avg_net_price', 'number_titleIV']]\n",
    "y = df[['rankingSortRank']]\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(X)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "true = []\n",
    "predicted = []\n",
    "\n",
    "for train_index, test_index in loocv.split(X):\n",
    "\n",
    "    X_train=X.loc[train_index]\n",
    "    X_test=X.loc[test_index]\n",
    "    y_train=y.loc[train_index]\n",
    "    y_test=y.loc[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    true.append(y_test['rankingSortRank'].values[0])\n",
    "    predicted.append(y_pred[0])\n",
    "    \n",
    "print(true)\n",
    "print(\"-----\")\n",
    "print(predicted)\n",
    "accuracy = accuracy_score(true, predicted)\n",
    "\n",
    "print(\"Accuracy:\", accuracy) # accuracy score for specific rank predicting, will do classification accuracy later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f1084",
   "metadata": {},
   "source": [
    "Using rank 50 as cutoff, higher than that is low-ranking, lower than that is high-ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814fb154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]\n",
      "-----\n",
      "[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "prediction below-----------------\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 35, 36, 37, 40, 41, 42, 45, 47, 48, 55, 56, 59, 60, 80, 87, 88, 94, 101, 103, 107, 112, 128, 132, 140, 141, 155, 161, 168, 170, 175, 177, 180, 187, 188, 191, 194, 197, 200, 201, 202, 206, 208, 211, 213, 215, 216, 217, 221, 222, 223, 224, 225, 226, 228, 230, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]\n",
      "-----\n",
      "[14, 21, 29, 32, 34, 38, 39, 43, 44, 46, 49, 50, 51, 52, 53, 54, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 102, 104, 105, 106, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 171, 172, 173, 174, 176, 178, 179, 181, 182, 183, 184, 185, 186, 189, 190, 192, 193, 195, 196, 198, 199, 203, 204, 205, 207, 209, 210, 212, 214, 218, 219, 220, 227, 229, 231, 232, 235, 237, 256, 272, 281, 285]\n"
     ]
    }
   ],
   "source": [
    "# use loocv to predict and get accuracy\n",
    "X = df[['act_avg', 'sat_avg','percent_receiving_aid',\n",
    "       'cost_after_aid', 'hs_gpa_avg','businessRepScore', 'tuition',\n",
    "       'engineeringRepScore','branches', 'admission_rate', \n",
    "       'ug_enrollment', 'percent_white', 'percent_black', 'percent_hispanic',\n",
    "       'percent_asian', 'percent_aian', 'percent_nhpi', 'percent_twoormore',\n",
    "       'percent_nra', 'percent_unknown', 'percent_parttime', 'avg_cost',\n",
    "       'instante_tuition', 'outstate_tuition', 'tuition_revenue_per',\n",
    "       'instructional_expenditure_per', 'avg_faculty_salary',\n",
    "       'ft_faculty_rate', 'avg_net_price', 'number_titleIV']]\n",
    "y = df[['rankingSortRank']]\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(X)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "true = []\n",
    "predicted = []\n",
    "\n",
    "high_df = []\n",
    "low_df = []\n",
    "high_predicted = []\n",
    "low_predicted = []\n",
    "\n",
    "for train_index, test_index in loocv.split(X):\n",
    "\n",
    "    X_train=X.loc[train_index]\n",
    "    X_test=X.loc[test_index]\n",
    "    y_train=y.loc[train_index]\n",
    "    y_test=y.loc[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    true.append(y_test['rankingSortRank'].values[0])\n",
    "    predicted.append(y_pred[0])\n",
    "    \n",
    "    #classifying into high and low\n",
    "    \n",
    "    if y_test['rankingSortRank'].values[0] < 50:\n",
    "        high_df.append(test_index[0])\n",
    "    else:\n",
    "        low_df.append(test_index[0])\n",
    "    if y_pred[0] < 50:\n",
    "        high_predicted.append(test_index[0])\n",
    "    else:\n",
    "        low_predicted.append(test_index[0])\n",
    "\n",
    "print(high_df)\n",
    "print(\"-----\")\n",
    "print(low_df)\n",
    "print(\"prediction below-----------------\")\n",
    "print(high_predicted)\n",
    "print(\"-----\")\n",
    "print(low_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b3ff0",
   "metadata": {},
   "source": [
    "After classifying into high and low categories, we want to see the accuracy for only divide into two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c79cb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_h = 0\n",
    "for i in high_df:\n",
    "    for j in high_predicted:\n",
    "        if i ==j:\n",
    "            correct_h = correct_h + 1 \n",
    "correct_h  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f5032b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_l = 0\n",
    "for i in low_df:\n",
    "    for j in low_predicted:\n",
    "        if i ==j:\n",
    "            correct_l = correct_l + 1 \n",
    "correct_l   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f050ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy rate for classifying into 2 categories\n",
    "(correct_h +correct_l)/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b39bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for submodel\n",
    "predicted_l = df.iloc[[14, 21, 29, 32, 34, 38, 39, 43, 44, 46, 49, 50, 51, 52, 53, 54, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 102, 104, 105, 106, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 171, 172, 173, 174, 176, 178, 179, 181, 182, 183, 184, 185, 186, 189, 190, 192, 193, 195, 196, 198, 199, 205, 207, 209, 210, 212, 214, 218, 220, 227, 229, 231, 232, 235, 237, 256, 272, 281, 285],:]\n",
    "predicted_l.to_csv('predicted_l.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40231b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for submodel\n",
    "predicted_h = df.iloc[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 35, 36, 37, 40, 41, 42, 45, 47, 48, 55, 56, 59, 60, 80, 87, 88, 94, 101, 103, 107, 112, 124, 128, 132, 140, 152, 155, 161, 168, 170, 175, 177, 180, 187, 188, 191, 194, 197, 200, 201, 202, 203, 204, 206, 208, 211, 213, 215, 216, 217, 219, 221, 222, 223, 224, 225, 226, 228, 230, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299],:]\n",
    "predicted_h.to_csv('predicted_h.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef96470",
   "metadata": {},
   "source": [
    "Now for dataset based on approach(1), we set 3 cutoffs and classify model into 4 categories directly, which we will compare with submodel later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e7ccd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]\n",
      "-----\n",
      "[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "-----\n",
      "[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]\n",
      "-----\n",
      "[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "prediction below-----------------\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 18, 19, 20, 22, 23, 24, 26, 35, 37, 56, 59, 88, 107, 132, 141, 155, 170, 175, 177, 180, 187, 188, 191, 194, 197, 200, 201, 202, 206, 208, 211, 213, 215, 216, 217, 221, 222, 223, 224, 225, 226, 228, 230, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]\n",
      "-----\n",
      "[13, 17, 25, 27, 28, 30, 31, 33, 36, 40, 41, 42, 45, 47, 48, 55, 60, 80, 87, 94, 101, 103, 112, 128, 140, 161, 168]\n",
      "-----\n",
      "[14, 21, 29, 32, 34, 38, 39, 43, 44, 46, 49, 50, 51, 52, 53, 54, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 82, 83, 85, 89, 90, 91, 92, 93, 96, 97, 100, 102, 104, 106, 108, 109, 110, 111, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 126, 127, 129, 130, 131, 133, 138, 139, 142, 143, 148, 149, 150, 153, 156, 157, 162, 167, 169, 172, 174, 179, 183, 184, 186, 192, 193, 196, 199, 204, 218, 231, 256]\n",
      "-----\n",
      "[70, 75, 81, 84, 86, 95, 98, 99, 105, 113, 118, 125, 134, 135, 136, 137, 144, 145, 146, 147, 151, 152, 154, 158, 159, 160, 163, 164, 165, 166, 171, 173, 176, 178, 181, 182, 185, 189, 190, 195, 198, 203, 205, 207, 209, 210, 212, 214, 219, 220, 227, 229, 232, 235, 237, 272, 281, 285]\n"
     ]
    }
   ],
   "source": [
    "# use loocv to predict and get accuracy through 3 cutoff in one model\n",
    "X = df[['act_avg', 'sat_avg','percent_receiving_aid',\n",
    "       'cost_after_aid', 'hs_gpa_avg','businessRepScore', 'tuition',\n",
    "       'engineeringRepScore','branches', 'admission_rate', \n",
    "       'ug_enrollment', 'percent_white', 'percent_black', 'percent_hispanic',\n",
    "       'percent_asian', 'percent_aian', 'percent_nhpi', 'percent_twoormore',\n",
    "       'percent_nra', 'percent_unknown', 'percent_parttime', 'avg_cost',\n",
    "       'instante_tuition', 'outstate_tuition', 'tuition_revenue_per',\n",
    "       'instructional_expenditure_per', 'avg_faculty_salary',\n",
    "       'ft_faculty_rate', 'avg_net_price', 'number_titleIV']]\n",
    "y = df[['rankingSortRank']]\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(X)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "true = []\n",
    "predicted = []\n",
    "\n",
    "high_df = []\n",
    "medium_high_df = []\n",
    "medium_low_df = []\n",
    "low_df = []\n",
    "high_predicted = []\n",
    "medium_high_predicted = []\n",
    "medium_low_predicted = []\n",
    "low_predicted = []\n",
    "\n",
    "\n",
    "for train_index, test_index in loocv.split(X):\n",
    "\n",
    "    X_train=X.loc[train_index]\n",
    "    X_test=X.loc[test_index]\n",
    "    y_train=y.loc[train_index]\n",
    "    y_test=y.loc[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    true.append(y_test['rankingSortRank'].values[0])\n",
    "    predicted.append(y_pred[0])\n",
    "    \n",
    "    #classifying into high and low\n",
    "    \n",
    "    if y_test['rankingSortRank'].values[0] < 50:\n",
    "        if y_test['rankingSortRank'].values[0] < 25:\n",
    "            high_df.append(test_index[0])\n",
    "        else:\n",
    "            medium_high_df.append(test_index[0])\n",
    "    else:\n",
    "        if y_test['rankingSortRank'].values[0] > 200:\n",
    "            low_df.append(test_index[0])\n",
    "        else:\n",
    "            medium_low_df.append(test_index[0])\n",
    "    if y_pred[0] < 50:\n",
    "        if y_pred[0] < 25:\n",
    "            high_predicted.append(test_index[0])\n",
    "        else:\n",
    "            medium_high_predicted.append(test_index[0])\n",
    "    else:\n",
    "        if y_pred[0] > 150:\n",
    "            low_predicted.append(test_index[0])\n",
    "        else:\n",
    "            medium_low_predicted.append(test_index[0])\n",
    "modelll=model.fit(X,y)\n",
    "print(high_df)\n",
    "print(\"-----\")\n",
    "print(medium_high_df)\n",
    "print(\"-----\")\n",
    "print(medium_low_df)\n",
    "print(\"-----\")\n",
    "print(low_df)\n",
    "print(\"prediction below-----------------\")\n",
    "print(high_predicted)\n",
    "print(\"-----\")\n",
    "print(medium_high_predicted)\n",
    "print(\"-----\")\n",
    "print(medium_low_predicted)\n",
    "print(\"-----\")\n",
    "print(low_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dee094",
   "metadata": {},
   "source": [
    "calculate accuracy for method that classify 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c50203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_h = 0\n",
    "for i in high_df:\n",
    "    for j in high_predicted:\n",
    "        if i ==j:\n",
    "            correct_h = correct_h + 1 \n",
    "correct_h  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd484a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_medium_h = 0\n",
    "for i in medium_high_df:\n",
    "    for j in medium_high_predicted:\n",
    "        if i ==j:\n",
    "            correct_medium_h = correct_medium_h + 1 \n",
    "correct_medium_h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d7e633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_medium_l = 0\n",
    "for i in medium_low_df:\n",
    "    for j in medium_low_predicted:\n",
    "        if i ==j:\n",
    "            correct_medium_l = correct_medium_l + 1 \n",
    "correct_medium_l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88486bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_l = 0\n",
    "for i in low_df:\n",
    "    for j in low_predicted:\n",
    "        if i ==j:\n",
    "            correct_l = correct_l + 1 \n",
    "correct_l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f012b80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy rate for classifying through 3 cutoff in 1 model\n",
    "(correct_h +correct_medium_h+correct_medium_l +correct_l)/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83faa1b9",
   "metadata": {},
   "source": [
    "Now we use dataset based on approach 2 to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39e5f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "-----\n",
      "[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "prediction below-----------------\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 35, 36, 37, 40, 41, 42, 45, 47, 48, 55, 56, 59, 60, 80, 87, 94, 101, 103, 112, 128, 140, 147, 161, 168]\n",
      "-----\n",
      "[14, 21, 29, 32, 34, 38, 39, 43, 44, 46, 49, 50, 51, 52, 53, 54, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n"
     ]
    }
   ],
   "source": [
    "# use loocv to predict and get accuracy\n",
    "X = df2[['act_avg', 'sat_avg','percent_receiving_aid',\n",
    "       'cost_after_aid', 'hs_gpa_avg','businessRepScore', 'tuition',\n",
    "       'engineeringRepScore','branches', 'admission_rate', \n",
    "       'ug_enrollment', 'percent_white', 'percent_black', 'percent_hispanic',\n",
    "       'percent_asian', 'percent_aian', 'percent_nhpi', 'percent_twoormore',\n",
    "       'percent_nra', 'percent_unknown', 'percent_parttime', 'avg_cost',\n",
    "       'instante_tuition', 'outstate_tuition', 'tuition_revenue_per',\n",
    "       'instructional_expenditure_per', 'avg_faculty_salary',\n",
    "       'ft_faculty_rate', 'avg_net_price', 'number_titleIV']]\n",
    "y = df2[['rankingSortRank']]\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(X)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "true = []\n",
    "predicted = []\n",
    "\n",
    "high_df = []\n",
    "low_df = []\n",
    "high_predicted = []\n",
    "low_predicted = []\n",
    "\n",
    "for train_index, test_index in loocv.split(X):\n",
    "\n",
    "    X_train=X.loc[train_index]\n",
    "    X_test=X.loc[test_index]\n",
    "    y_train=y.loc[train_index]\n",
    "    y_test=y.loc[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    true.append(y_test['rankingSortRank'].values[0])\n",
    "    predicted.append(y_pred[0])\n",
    "    \n",
    "    #classifying into high and low\n",
    "    \n",
    "    if y_test['rankingSortRank'].values[0] < 50:\n",
    "        high_df.append(test_index[0])\n",
    "    else:\n",
    "        low_df.append(test_index[0])\n",
    "    if y_pred[0] < 50:\n",
    "        high_predicted.append(test_index[0])\n",
    "    else:\n",
    "        low_predicted.append(test_index[0])\n",
    "\n",
    "print(high_df)\n",
    "print(\"-----\")\n",
    "print(low_df)\n",
    "print(\"prediction below-----------------\")\n",
    "print(high_predicted)\n",
    "print(\"-----\")\n",
    "print(low_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for submodel-drop -1 ranking \n",
    "predicted_h = df.iloc[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 35, 36, 37, 40, 41, 42, 45, 47, 48, 55, 56, 59, 60, 80, 85, 87, 94, 101, 103, 112, 128, 140, 147, 161, 168],:]\n",
    "predicted_h.to_csv('predicted_h_230rows.csv')\n",
    "predicted_l = df.iloc[[14, 21, 29, 32, 34, 38, 39, 43, 44, 46, 49, 50, 51, 52, 53, 54, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229],:]\n",
    "predicted_l.to_csv('predicted_l_230rows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca2e5381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_h = 0\n",
    "for i in high_df:\n",
    "    for j in high_predicted:\n",
    "        if i ==j:\n",
    "            correct_h = correct_h + 1 \n",
    "correct_h   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "397d6432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_l = 0\n",
    "for i in low_df:\n",
    "    for j in low_predicted:\n",
    "        if i ==j:\n",
    "            correct_l = correct_l + 1 \n",
    "correct_l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f426cc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8826086956521739"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy rate for classifying through 3 cutoff in 1 model\n",
    "(correct_h +correct_l)/df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "274aa6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "-----\n",
      "[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "-----\n",
      "[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]\n",
      "-----\n",
      "[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "prediction below-----------------\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 24, 26, 35, 37, 56, 59, 147]\n",
      "-----\n",
      "[17, 25, 27, 28, 30, 31, 33, 36, 40, 41, 42, 45, 47, 48, 55, 60, 80, 87, 94, 101, 103, 112, 128, 140, 161, 168]\n",
      "-----\n",
      "[14, 21, 29, 32, 34, 38, 39, 43, 44, 46, 49, 50, 51, 52, 53, 54, 57, 58, 61, 62, 63, 64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 82, 83, 85, 86, 89, 90, 91, 92, 93, 96, 97, 100, 102, 104, 108, 109, 110, 111, 114, 115, 116, 117, 119, 122, 123, 124, 126, 127, 129, 130, 131, 133, 138, 139, 142, 143, 148, 149, 150, 153, 155, 156, 157, 162, 167, 169, 172, 174, 180, 182, 183, 186, 187, 192, 193, 196, 199, 204, 211, 218]\n",
      "-----\n",
      "[65, 70, 75, 81, 84, 88, 95, 98, 99, 105, 106, 107, 113, 118, 120, 121, 125, 132, 134, 135, 136, 137, 141, 144, 145, 146, 151, 152, 154, 158, 159, 160, 163, 164, 165, 166, 170, 171, 173, 175, 176, 177, 178, 179, 181, 184, 185, 188, 189, 190, 191, 194, 195, 197, 198, 200, 201, 202, 203, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n"
     ]
    }
   ],
   "source": [
    "# use loocv to predict and get accuracy through 3 cutoff in one model\n",
    "X = df2[['act_avg', 'sat_avg','percent_receiving_aid',\n",
    "       'cost_after_aid', 'hs_gpa_avg','businessRepScore', 'tuition',\n",
    "       'engineeringRepScore','branches', 'admission_rate', \n",
    "       'ug_enrollment', 'percent_white', 'percent_black', 'percent_hispanic',\n",
    "       'percent_asian', 'percent_aian', 'percent_nhpi', 'percent_twoormore',\n",
    "       'percent_nra', 'percent_unknown', 'percent_parttime', 'avg_cost',\n",
    "       'instante_tuition', 'outstate_tuition', 'tuition_revenue_per',\n",
    "       'instructional_expenditure_per', 'avg_faculty_salary',\n",
    "       'ft_faculty_rate', 'avg_net_price', 'number_titleIV']]\n",
    "y = df2[['rankingSortRank']]\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(X)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "true = []\n",
    "predicted = []\n",
    "\n",
    "high_df = []\n",
    "medium_high_df = []\n",
    "medium_low_df = []\n",
    "low_df = []\n",
    "high_predicted = []\n",
    "medium_high_predicted = []\n",
    "medium_low_predicted = []\n",
    "low_predicted = []\n",
    "\n",
    "\n",
    "for train_index, test_index in loocv.split(X):\n",
    "\n",
    "    X_train=X.loc[train_index]\n",
    "    X_test=X.loc[test_index]\n",
    "    y_train=y.loc[train_index]\n",
    "    y_test=y.loc[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    true.append(y_test['rankingSortRank'].values[0])\n",
    "    predicted.append(y_pred[0])\n",
    "    \n",
    "    #classifying into high and low\n",
    "    \n",
    "    if y_test['rankingSortRank'].values[0] < 50:\n",
    "        if y_test['rankingSortRank'].values[0] < 25:\n",
    "            high_df.append(test_index[0])\n",
    "        else:\n",
    "            medium_high_df.append(test_index[0])\n",
    "    else:\n",
    "        if y_test['rankingSortRank'].values[0] > 200:\n",
    "            low_df.append(test_index[0])\n",
    "        else:\n",
    "            medium_low_df.append(test_index[0])\n",
    "    if y_pred[0] < 50:\n",
    "        if y_pred[0] < 25:\n",
    "            high_predicted.append(test_index[0])\n",
    "        else:\n",
    "            medium_high_predicted.append(test_index[0])\n",
    "    else:\n",
    "        if y_pred[0] > 150:\n",
    "            low_predicted.append(test_index[0])\n",
    "        else:\n",
    "            medium_low_predicted.append(test_index[0])\n",
    "\n",
    "            \n",
    "print(high_df)\n",
    "print(\"-----\")\n",
    "print(medium_high_df)\n",
    "print(\"-----\")\n",
    "print(medium_low_df)\n",
    "print(\"-----\")\n",
    "print(low_df)\n",
    "print(\"prediction below-----------------\")\n",
    "print(high_predicted)\n",
    "print(\"-----\")\n",
    "print(medium_high_predicted)\n",
    "print(\"-----\")\n",
    "print(medium_low_predicted)\n",
    "print(\"-----\")\n",
    "print(low_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8b7221d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_h = 0\n",
    "for i in high_df:\n",
    "    for j in high_predicted:\n",
    "        if i ==j:\n",
    "            correct_h = correct_h + 1 \n",
    "correct_h  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e27ca2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_medium_h = 0\n",
    "for i in medium_high_df:\n",
    "    for j in medium_high_predicted:\n",
    "        if i ==j:\n",
    "            correct_medium_h = correct_medium_h + 1 \n",
    "correct_medium_h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b34b3e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_medium_l = 0\n",
    "for i in medium_low_df:\n",
    "    for j in medium_low_predicted:\n",
    "        if i ==j:\n",
    "            correct_medium_l = correct_medium_l + 1 \n",
    "correct_medium_l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf9a95c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_l = 0\n",
    "for i in low_df:\n",
    "    for j in low_predicted:\n",
    "        if i ==j:\n",
    "            correct_l = correct_l + 1 \n",
    "correct_l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3f83221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4633333333333333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy rate for classifying through 3 cutoff in 1 model - drop invalid ranking\n",
    "(correct_h +correct_medium_h+correct_medium_l +correct_l)/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d41354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd291e64",
   "metadata": {},
   "source": [
    "### Developing the Plain Model\n",
    "*By Peggy Han*\n",
    "\n",
    "We developed a plain model that simply uses all variables as predictors with no transformation or interaction terms included to serve as a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85a783b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/7c21x7qj4fv7flh8p9m2wwn00000gn/T/ipykernel_86468/2483564295.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df = df.fillna(df.mean())\n"
     ]
    }
   ],
   "source": [
    "# Plain Model\n",
    "df = pd.read_csv('newdata.csv')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Ranking of -1 indicates the school is ranked in the range of #231 - #300\n",
    "# So I replaced them all with ranking of 300\n",
    "df['rankingSortRank'] = df['rankingSortRank'].replace(-1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "029eca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categories for universities based on the original rankings\n",
    "bins = [0, 75, 150, 225, float('inf')]\n",
    "labels = ['high', 'med_high', 'med_low', 'low']\n",
    "df['categories'] = pd.cut(df['rankingSortRank'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0248419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['act_avg', 'percent_receiving_aid', 'cost_after_aid',\n",
    "        'hs_gpa_avg', 'businessRepScore', 'tuition',\n",
    "       'engineeringRepScore','branches', 'region', 'admission_rate',\n",
    "       'ug_enrollment', 'percent_white', 'percent_black', 'percent_hispanic',\n",
    "       'percent_asian', 'percent_aian', 'percent_nhpi', 'percent_twoormore',\n",
    "       'percent_nra', 'percent_unknown', 'percent_parttime', 'avg_cost',\n",
    "       'instante_tuition', 'outstate_tuition', 'tuition_revenue_per',\n",
    "       'instructional_expenditure_per', 'avg_faculty_salary',\n",
    "       'ft_faculty_rate', 'avg_net_price', 'number_titleIV', 'sat_avg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575dc87",
   "metadata": {},
   "source": [
    "#### Using all data for train and predict on trian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71750fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>rankingSortRank</td> <th>  R-squared:         </th> <td>   0.894</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.881</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   72.63</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 14 Mar 2023</td> <th>  Prob (F-statistic):</th> <td>3.63e-112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:31:17</td>     <th>  Log-Likelihood:    </th> <td> -1464.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   300</td>      <th>  AIC:               </th> <td>   2992.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   268</td>      <th>  BIC:               </th> <td>   3110.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    31</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                   <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                     <td>-9885.2509</td> <td> 2.52e+04</td> <td>   -0.393</td> <td> 0.695</td> <td>-5.95e+04</td> <td> 3.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>act_avg</th>                       <td>   -3.7605</td> <td>    1.803</td> <td>   -2.086</td> <td> 0.038</td> <td>   -7.309</td> <td>   -0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_receiving_aid</th>         <td>    0.2265</td> <td>    0.249</td> <td>    0.911</td> <td> 0.363</td> <td>   -0.263</td> <td>    0.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cost_after_aid</th>                <td>    0.0008</td> <td>    0.001</td> <td>    1.580</td> <td> 0.115</td> <td>   -0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hs_gpa_avg</th>                    <td>  -38.3910</td> <td>   13.057</td> <td>   -2.940</td> <td> 0.004</td> <td>  -64.099</td> <td>  -12.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>businessRepScore</th>              <td>  -21.4875</td> <td>    5.852</td> <td>   -3.672</td> <td> 0.000</td> <td>  -33.010</td> <td>   -9.965</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tuition</th>                       <td>    0.0009</td> <td>    0.001</td> <td>    0.857</td> <td> 0.392</td> <td>   -0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>engineeringRepScore</th>           <td>    4.3522</td> <td>    5.223</td> <td>    0.833</td> <td> 0.405</td> <td>   -5.932</td> <td>   14.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>branches</th>                      <td>   -1.7699</td> <td>    1.304</td> <td>   -1.358</td> <td> 0.176</td> <td>   -4.336</td> <td>    0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>region</th>                        <td>    2.1616</td> <td>    1.319</td> <td>    1.639</td> <td> 0.102</td> <td>   -0.434</td> <td>    4.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>admission_rate</th>                <td>    7.9172</td> <td>   16.567</td> <td>    0.478</td> <td> 0.633</td> <td>  -24.700</td> <td>   40.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ug_enrollment</th>                 <td>   -0.0014</td> <td>    0.001</td> <td>   -2.659</td> <td> 0.008</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_white</th>                 <td>  1.05e+04</td> <td> 2.52e+04</td> <td>    0.417</td> <td> 0.677</td> <td>-3.91e+04</td> <td> 6.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_black</th>                 <td> 1.056e+04</td> <td> 2.52e+04</td> <td>    0.419</td> <td> 0.675</td> <td> -3.9e+04</td> <td> 6.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_hispanic</th>              <td> 1.051e+04</td> <td> 2.52e+04</td> <td>    0.417</td> <td> 0.677</td> <td>-3.91e+04</td> <td> 6.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_asian</th>                 <td> 1.046e+04</td> <td> 2.52e+04</td> <td>    0.415</td> <td> 0.678</td> <td>-3.91e+04</td> <td>    6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_aian</th>                  <td> 9559.7019</td> <td> 2.52e+04</td> <td>    0.379</td> <td> 0.705</td> <td>-4.01e+04</td> <td> 5.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_nhpi</th>                  <td> 8018.1592</td> <td> 2.52e+04</td> <td>    0.319</td> <td> 0.750</td> <td>-4.15e+04</td> <td> 5.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_twoormore</th>             <td> 1.089e+04</td> <td> 2.52e+04</td> <td>    0.432</td> <td> 0.666</td> <td>-3.87e+04</td> <td> 6.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_nra</th>                   <td> 1.051e+04</td> <td> 2.52e+04</td> <td>    0.418</td> <td> 0.677</td> <td>-3.91e+04</td> <td> 6.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_unknown</th>               <td> 1.068e+04</td> <td> 2.52e+04</td> <td>    0.424</td> <td> 0.672</td> <td>-3.89e+04</td> <td> 6.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_parttime</th>              <td>  225.2990</td> <td>   31.535</td> <td>    7.144</td> <td> 0.000</td> <td>  163.211</td> <td>  287.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_cost</th>                      <td>   -0.0026</td> <td>    0.001</td> <td>   -2.468</td> <td> 0.014</td> <td>   -0.005</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instante_tuition</th>              <td>    0.0037</td> <td>    0.001</td> <td>    3.322</td> <td> 0.001</td> <td>    0.001</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outstate_tuition</th>              <td>   -0.0029</td> <td>    0.001</td> <td>   -2.749</td> <td> 0.006</td> <td>   -0.005</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tuition_revenue_per</th>           <td>   -0.0001</td> <td>    0.001</td> <td>   -0.180</td> <td> 0.858</td> <td>   -0.002</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instructional_expenditure_per</th> <td> 9.454e-05</td> <td>    0.000</td> <td>    0.444</td> <td> 0.657</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_faculty_salary</th>            <td>   -0.0058</td> <td>    0.002</td> <td>   -3.263</td> <td> 0.001</td> <td>   -0.009</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ft_faculty_rate</th>               <td>   -2.7051</td> <td>   12.045</td> <td>   -0.225</td> <td> 0.822</td> <td>  -26.420</td> <td>   21.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_net_price</th>                 <td>   -0.0018</td> <td>    0.001</td> <td>   -2.572</td> <td> 0.011</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>number_titleIV</th>                <td>    0.0142</td> <td>    0.006</td> <td>    2.559</td> <td> 0.011</td> <td>    0.003</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat_avg</th>                       <td>   -0.0820</td> <td>    0.044</td> <td>   -1.866</td> <td> 0.063</td> <td>   -0.168</td> <td>    0.005</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 8.964</td> <th>  Durbin-Watson:     </th> <td>   1.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.011</td> <th>  Jarque-Bera (JB):  </th> <td>   8.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.384</td> <th>  Prob(JB):          </th> <td>  0.0115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.354</td> <th>  Cond. No.          </th> <td>3.40e+09</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.4e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        rankingSortRank   R-squared:                       0.894\n",
       "Model:                            OLS   Adj. R-squared:                  0.881\n",
       "Method:                 Least Squares   F-statistic:                     72.63\n",
       "Date:                Tue, 14 Mar 2023   Prob (F-statistic):          3.63e-112\n",
       "Time:                        15:31:17   Log-Likelihood:                -1464.0\n",
       "No. Observations:                 300   AIC:                             2992.\n",
       "Df Residuals:                     268   BIC:                             3110.\n",
       "Df Model:                          31                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================================\n",
       "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------\n",
       "Intercept                     -9885.2509   2.52e+04     -0.393      0.695   -5.95e+04    3.97e+04\n",
       "act_avg                          -3.7605      1.803     -2.086      0.038      -7.309      -0.212\n",
       "percent_receiving_aid             0.2265      0.249      0.911      0.363      -0.263       0.716\n",
       "cost_after_aid                    0.0008      0.001      1.580      0.115      -0.000       0.002\n",
       "hs_gpa_avg                      -38.3910     13.057     -2.940      0.004     -64.099     -12.683\n",
       "businessRepScore                -21.4875      5.852     -3.672      0.000     -33.010      -9.965\n",
       "tuition                           0.0009      0.001      0.857      0.392      -0.001       0.003\n",
       "engineeringRepScore               4.3522      5.223      0.833      0.405      -5.932      14.636\n",
       "branches                         -1.7699      1.304     -1.358      0.176      -4.336       0.797\n",
       "region                            2.1616      1.319      1.639      0.102      -0.434       4.758\n",
       "admission_rate                    7.9172     16.567      0.478      0.633     -24.700      40.535\n",
       "ug_enrollment                    -0.0014      0.001     -2.659      0.008      -0.002      -0.000\n",
       "percent_white                   1.05e+04   2.52e+04      0.417      0.677   -3.91e+04    6.01e+04\n",
       "percent_black                  1.056e+04   2.52e+04      0.419      0.675    -3.9e+04    6.01e+04\n",
       "percent_hispanic               1.051e+04   2.52e+04      0.417      0.677   -3.91e+04    6.01e+04\n",
       "percent_asian                  1.046e+04   2.52e+04      0.415      0.678   -3.91e+04       6e+04\n",
       "percent_aian                   9559.7019   2.52e+04      0.379      0.705   -4.01e+04    5.92e+04\n",
       "percent_nhpi                   8018.1592   2.52e+04      0.319      0.750   -4.15e+04    5.76e+04\n",
       "percent_twoormore              1.089e+04   2.52e+04      0.432      0.666   -3.87e+04    6.05e+04\n",
       "percent_nra                    1.051e+04   2.52e+04      0.418      0.677   -3.91e+04    6.01e+04\n",
       "percent_unknown                1.068e+04   2.52e+04      0.424      0.672   -3.89e+04    6.03e+04\n",
       "percent_parttime                225.2990     31.535      7.144      0.000     163.211     287.387\n",
       "avg_cost                         -0.0026      0.001     -2.468      0.014      -0.005      -0.001\n",
       "instante_tuition                  0.0037      0.001      3.322      0.001       0.001       0.006\n",
       "outstate_tuition                 -0.0029      0.001     -2.749      0.006      -0.005      -0.001\n",
       "tuition_revenue_per              -0.0001      0.001     -0.180      0.858      -0.002       0.001\n",
       "instructional_expenditure_per  9.454e-05      0.000      0.444      0.657      -0.000       0.001\n",
       "avg_faculty_salary               -0.0058      0.002     -3.263      0.001      -0.009      -0.002\n",
       "ft_faculty_rate                  -2.7051     12.045     -0.225      0.822     -26.420      21.010\n",
       "avg_net_price                    -0.0018      0.001     -2.572      0.011      -0.003      -0.000\n",
       "number_titleIV                    0.0142      0.006      2.559      0.011       0.003       0.025\n",
       "sat_avg                          -0.0820      0.044     -1.866      0.063      -0.168       0.005\n",
       "==============================================================================\n",
       "Omnibus:                        8.964   Durbin-Watson:                   1.495\n",
       "Prob(Omnibus):                  0.011   Jarque-Bera (JB):                8.930\n",
       "Skew:                           0.384   Prob(JB):                       0.0115\n",
       "Kurtosis:                       3.354   Cond. No.                     3.40e+09\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.4e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model = sm.ols(formula = 'rankingSortRank~' + '+'.join(X.columns),data = df).fit()\n",
    "plain_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b25f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = plain_model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fc95a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for universities based on the predicted rankings\n",
    "bins = [-float('inf'), 75, 150, 225, float('inf')]\n",
    "labels = ['high', 'med_high', 'med_low', 'low']\n",
    "categories = pd.cut(prediction, bins=bins, labels=labels)\n",
    "df['pred_category'] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07aaf7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the accuracy using the number of matching rankings\n",
    "num_matches = df['categories'].eq(df['pred_category']).value_counts(normalize=True)[True] * len(df)\n",
    "num_matches/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01c97c",
   "metadata": {},
   "source": [
    "#### Splitting into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89917179",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.sample(n=50, random_state=1)\n",
    "train = df.drop(test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34a859cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>rankingSortRank</td> <th>  R-squared:         </th> <td>   0.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   60.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 14 Mar 2023</td> <th>  Prob (F-statistic):</th> <td>5.47e-90</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:40:58</td>     <th>  Log-Likelihood:    </th> <td> -1219.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   250</td>      <th>  AIC:               </th> <td>   2503.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   218</td>      <th>  BIC:               </th> <td>   2616.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    31</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                   <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                     <td> 2.129e+04</td> <td>  2.9e+04</td> <td>    0.735</td> <td> 0.463</td> <td>-3.58e+04</td> <td> 7.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>act_avg</th>                       <td>   -2.9711</td> <td>    2.026</td> <td>   -1.466</td> <td> 0.144</td> <td>   -6.964</td> <td>    1.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_receiving_aid</th>         <td>    0.1447</td> <td>    0.275</td> <td>    0.526</td> <td> 0.600</td> <td>   -0.398</td> <td>    0.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cost_after_aid</th>                <td>    0.0006</td> <td>    0.001</td> <td>    1.018</td> <td> 0.310</td> <td>   -0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hs_gpa_avg</th>                    <td>  -33.4451</td> <td>   14.271</td> <td>   -2.344</td> <td> 0.020</td> <td>  -61.571</td> <td>   -5.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>businessRepScore</th>              <td>  -18.0375</td> <td>    6.467</td> <td>   -2.789</td> <td> 0.006</td> <td>  -30.783</td> <td>   -5.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tuition</th>                       <td>    0.0008</td> <td>    0.001</td> <td>    0.732</td> <td> 0.465</td> <td>   -0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>engineeringRepScore</th>           <td>    5.9757</td> <td>    5.825</td> <td>    1.026</td> <td> 0.306</td> <td>   -5.504</td> <td>   17.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>branches</th>                      <td>   -4.9690</td> <td>    2.011</td> <td>   -2.471</td> <td> 0.014</td> <td>   -8.933</td> <td>   -1.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>region</th>                        <td>    1.3090</td> <td>    1.506</td> <td>    0.869</td> <td> 0.386</td> <td>   -1.659</td> <td>    4.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>admission_rate</th>                <td>   22.8263</td> <td>   18.443</td> <td>    1.238</td> <td> 0.217</td> <td>  -13.524</td> <td>   59.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ug_enrollment</th>                 <td>   -0.0013</td> <td>    0.001</td> <td>   -2.234</td> <td> 0.026</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_white</th>                 <td>-2.069e+04</td> <td>  2.9e+04</td> <td>   -0.714</td> <td> 0.476</td> <td>-7.78e+04</td> <td> 3.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_black</th>                 <td>-2.064e+04</td> <td>  2.9e+04</td> <td>   -0.712</td> <td> 0.477</td> <td>-7.77e+04</td> <td> 3.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_hispanic</th>              <td>-2.067e+04</td> <td>  2.9e+04</td> <td>   -0.713</td> <td> 0.476</td> <td>-7.78e+04</td> <td> 3.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_asian</th>                 <td>-2.072e+04</td> <td>  2.9e+04</td> <td>   -0.715</td> <td> 0.475</td> <td>-7.78e+04</td> <td> 3.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_aian</th>                  <td> -2.16e+04</td> <td>  2.9e+04</td> <td>   -0.745</td> <td> 0.457</td> <td>-7.87e+04</td> <td> 3.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_nhpi</th>                  <td>-2.303e+04</td> <td>  2.9e+04</td> <td>   -0.793</td> <td> 0.429</td> <td>-8.03e+04</td> <td> 3.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_twoormore</th>             <td>-2.016e+04</td> <td>  2.9e+04</td> <td>   -0.696</td> <td> 0.487</td> <td>-7.73e+04</td> <td>  3.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_nra</th>                   <td>-2.066e+04</td> <td>  2.9e+04</td> <td>   -0.713</td> <td> 0.476</td> <td>-7.78e+04</td> <td> 3.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_unknown</th>               <td>-2.053e+04</td> <td>  2.9e+04</td> <td>   -0.709</td> <td> 0.479</td> <td>-7.76e+04</td> <td> 3.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_parttime</th>              <td>  227.4443</td> <td>   34.568</td> <td>    6.580</td> <td> 0.000</td> <td>  159.315</td> <td>  295.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_cost</th>                      <td>   -0.0018</td> <td>    0.001</td> <td>   -1.333</td> <td> 0.184</td> <td>   -0.004</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instante_tuition</th>              <td>    0.0030</td> <td>    0.001</td> <td>    2.178</td> <td> 0.030</td> <td>    0.000</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outstate_tuition</th>              <td>   -0.0028</td> <td>    0.001</td> <td>   -2.531</td> <td> 0.012</td> <td>   -0.005</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tuition_revenue_per</th>           <td>   -0.0005</td> <td>    0.001</td> <td>   -0.599</td> <td> 0.550</td> <td>   -0.002</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instructional_expenditure_per</th> <td> 7.235e-05</td> <td>    0.000</td> <td>    0.263</td> <td> 0.793</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_faculty_salary</th>            <td>   -0.0069</td> <td>    0.002</td> <td>   -3.119</td> <td> 0.002</td> <td>   -0.011</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ft_faculty_rate</th>               <td>   -2.1890</td> <td>   14.568</td> <td>   -0.150</td> <td> 0.881</td> <td>  -30.902</td> <td>   26.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_net_price</th>                 <td>   -0.0019</td> <td>    0.001</td> <td>   -2.449</td> <td> 0.015</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>number_titleIV</th>                <td>    0.0106</td> <td>    0.006</td> <td>    1.720</td> <td> 0.087</td> <td>   -0.002</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat_avg</th>                       <td>   -0.1057</td> <td>    0.049</td> <td>   -2.142</td> <td> 0.033</td> <td>   -0.203</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.094</td> <th>  Durbin-Watson:     </th> <td>   1.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.029</td> <th>  Jarque-Bera (JB):  </th> <td>   6.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.384</td> <th>  Prob(JB):          </th> <td>  0.0325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.263</td> <th>  Cond. No.          </th> <td>3.51e+09</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.51e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        rankingSortRank   R-squared:                       0.896\n",
       "Model:                            OLS   Adj. R-squared:                  0.881\n",
       "Method:                 Least Squares   F-statistic:                     60.41\n",
       "Date:                Tue, 14 Mar 2023   Prob (F-statistic):           5.47e-90\n",
       "Time:                        15:40:58   Log-Likelihood:                -1219.5\n",
       "No. Observations:                 250   AIC:                             2503.\n",
       "Df Residuals:                     218   BIC:                             2616.\n",
       "Df Model:                          31                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================================\n",
       "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------\n",
       "Intercept                      2.129e+04    2.9e+04      0.735      0.463   -3.58e+04    7.84e+04\n",
       "act_avg                          -2.9711      2.026     -1.466      0.144      -6.964       1.022\n",
       "percent_receiving_aid             0.1447      0.275      0.526      0.600      -0.398       0.687\n",
       "cost_after_aid                    0.0006      0.001      1.018      0.310      -0.001       0.002\n",
       "hs_gpa_avg                      -33.4451     14.271     -2.344      0.020     -61.571      -5.319\n",
       "businessRepScore                -18.0375      6.467     -2.789      0.006     -30.783      -5.292\n",
       "tuition                           0.0008      0.001      0.732      0.465      -0.001       0.003\n",
       "engineeringRepScore               5.9757      5.825      1.026      0.306      -5.504      17.455\n",
       "branches                         -4.9690      2.011     -2.471      0.014      -8.933      -1.005\n",
       "region                            1.3090      1.506      0.869      0.386      -1.659       4.277\n",
       "admission_rate                   22.8263     18.443      1.238      0.217     -13.524      59.177\n",
       "ug_enrollment                    -0.0013      0.001     -2.234      0.026      -0.002      -0.000\n",
       "percent_white                 -2.069e+04    2.9e+04     -0.714      0.476   -7.78e+04    3.64e+04\n",
       "percent_black                 -2.064e+04    2.9e+04     -0.712      0.477   -7.77e+04    3.65e+04\n",
       "percent_hispanic              -2.067e+04    2.9e+04     -0.713      0.476   -7.78e+04    3.64e+04\n",
       "percent_asian                 -2.072e+04    2.9e+04     -0.715      0.475   -7.78e+04    3.64e+04\n",
       "percent_aian                   -2.16e+04    2.9e+04     -0.745      0.457   -7.87e+04    3.55e+04\n",
       "percent_nhpi                  -2.303e+04    2.9e+04     -0.793      0.429   -8.03e+04    3.42e+04\n",
       "percent_twoormore             -2.016e+04    2.9e+04     -0.696      0.487   -7.73e+04     3.7e+04\n",
       "percent_nra                   -2.066e+04    2.9e+04     -0.713      0.476   -7.78e+04    3.64e+04\n",
       "percent_unknown               -2.053e+04    2.9e+04     -0.709      0.479   -7.76e+04    3.66e+04\n",
       "percent_parttime                227.4443     34.568      6.580      0.000     159.315     295.574\n",
       "avg_cost                         -0.0018      0.001     -1.333      0.184      -0.004       0.001\n",
       "instante_tuition                  0.0030      0.001      2.178      0.030       0.000       0.006\n",
       "outstate_tuition                 -0.0028      0.001     -2.531      0.012      -0.005      -0.001\n",
       "tuition_revenue_per              -0.0005      0.001     -0.599      0.550      -0.002       0.001\n",
       "instructional_expenditure_per  7.235e-05      0.000      0.263      0.793      -0.000       0.001\n",
       "avg_faculty_salary               -0.0069      0.002     -3.119      0.002      -0.011      -0.003\n",
       "ft_faculty_rate                  -2.1890     14.568     -0.150      0.881     -30.902      26.524\n",
       "avg_net_price                    -0.0019      0.001     -2.449      0.015      -0.003      -0.000\n",
       "number_titleIV                    0.0106      0.006      1.720      0.087      -0.002       0.023\n",
       "sat_avg                          -0.1057      0.049     -2.142      0.033      -0.203      -0.008\n",
       "==============================================================================\n",
       "Omnibus:                        7.094   Durbin-Watson:                   1.490\n",
       "Prob(Omnibus):                  0.029   Jarque-Bera (JB):                6.851\n",
       "Skew:                           0.384   Prob(JB):                       0.0325\n",
       "Kurtosis:                       3.263   Cond. No.                     3.51e+09\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.51e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model_1 = sm.ols(formula = 'rankingSortRank~' + '+'.join(X.columns),data = train).fit()\n",
    "plain_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbb633ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = plain_model_1.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5878065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for universities based on the predicted rankings\n",
    "bins = [-float('inf'), 75, 150, 225, float('inf')]\n",
    "labels = ['high', 'med_high', 'med_low', 'low']\n",
    "categories = pd.cut(y_pred, bins=bins, labels=labels)\n",
    "test['pred_category'] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c58db35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the accuracy using the number of matching rankings\n",
    "matches = test['categories'].eq(test['pred_category']).value_counts(normalize=True)[True] * len(df)\n",
    "matches/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444e611",
   "metadata": {},
   "source": [
    "### Code fitting the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cae72f",
   "metadata": {},
   "source": [
    "Put the code(s) that fit the final model(s) in separate cell(s), i.e., the code with the `.ols()` or `.logit()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ced8db",
   "metadata": {},
   "source": [
    "This is model fit for main model and we use df2 for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a23c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['act_avg', 'sat_avg','percent_receiving_aid',\n",
    "       'cost_after_aid', 'hs_gpa_avg','businessRepScore', 'tuition',\n",
    "       'engineeringRepScore','branches', 'admission_rate', \n",
    "       'ug_enrollment', 'percent_white', 'percent_black', 'percent_hispanic',\n",
    "       'percent_asian', 'percent_aian', 'percent_nhpi', 'percent_twoormore',\n",
    "       'percent_nra', 'percent_unknown', 'percent_parttime', 'avg_cost',\n",
    "       'instante_tuition', 'outstate_tuition', 'tuition_revenue_per',\n",
    "       'instructional_expenditure_per', 'avg_faculty_salary',\n",
    "       'ft_faculty_rate', 'avg_net_price', 'number_titleIV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed4e092b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>rankingSortRank</td> <th>  R-squared:         </th> <td>   0.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   76.97</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 14 Mar 2023</td> <th>  Prob (F-statistic):</th> <td>3.83e-95</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:24:34</td>     <th>  Log-Likelihood:    </th> <td> -982.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   230</td>      <th>  AIC:               </th> <td>   2034.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   195</td>      <th>  BIC:               </th> <td>   2155.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    34</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                         <td>-1.713e+04</td> <td> 1.62e+04</td> <td>   -1.056</td> <td> 0.292</td> <td>-4.91e+04</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>act_avg</th>                           <td>   -5.3288</td> <td>    1.760</td> <td>   -3.028</td> <td> 0.003</td> <td>   -8.799</td> <td>   -1.858</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>admission_rate</th>                    <td>  217.1199</td> <td>   68.479</td> <td>    3.171</td> <td> 0.002</td> <td>   82.064</td> <td>  352.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>act_avg:admission_rate</th>            <td>   -2.7438</td> <td>    2.669</td> <td>   -1.028</td> <td> 0.305</td> <td>   -8.008</td> <td>    2.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>businessRepScore</th>                  <td>    4.0317</td> <td>   10.578</td> <td>    0.381</td> <td> 0.704</td> <td>  -16.830</td> <td>   24.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>businessRepScore:admission_rate</th>   <td>  -42.8449</td> <td>   13.045</td> <td>   -3.285</td> <td> 0.001</td> <td>  -68.571</td> <td>  -17.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_parttime</th>                  <td>  275.8040</td> <td>   63.612</td> <td>    4.336</td> <td> 0.000</td> <td>  150.349</td> <td>  401.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_cost</th>                          <td>   -0.0019</td> <td>    0.001</td> <td>   -2.339</td> <td> 0.020</td> <td>   -0.004</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_parttime:avg_cost</th>         <td>   -0.0035</td> <td>    0.002</td> <td>   -2.140</td> <td> 0.034</td> <td>   -0.007</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instante_tuition</th>                  <td>    0.0026</td> <td>    0.001</td> <td>    2.747</td> <td> 0.007</td> <td>    0.001</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instante_tuition:businessRepScore</th> <td> 4.524e-05</td> <td>    0.000</td> <td>    0.242</td> <td> 0.809</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat_avg</th>                           <td>   -0.0208</td> <td>    0.030</td> <td>   -0.701</td> <td> 0.484</td> <td>   -0.079</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_receiving_aid</th>             <td>   -0.0003</td> <td>    0.145</td> <td>   -0.002</td> <td> 0.998</td> <td>   -0.286</td> <td>    0.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cost_after_aid</th>                    <td>    0.0006</td> <td>    0.000</td> <td>    1.852</td> <td> 0.065</td> <td>-3.78e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hs_gpa_avg</th>                        <td>  -11.5985</td> <td>    9.536</td> <td>   -1.216</td> <td> 0.225</td> <td>  -30.406</td> <td>    7.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tuition</th>                           <td>    0.0007</td> <td>    0.001</td> <td>    0.796</td> <td> 0.427</td> <td>   -0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>engineeringRepScore</th>               <td>   -1.9759</td> <td>    3.550</td> <td>   -0.557</td> <td> 0.578</td> <td>   -8.978</td> <td>    5.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>branches</th>                          <td>   -0.6165</td> <td>    0.740</td> <td>   -0.833</td> <td> 0.406</td> <td>   -2.076</td> <td>    0.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ug_enrollment</th>                     <td>   -0.0005</td> <td>    0.000</td> <td>   -1.418</td> <td> 0.158</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_white</th>                     <td>  1.75e+04</td> <td> 1.62e+04</td> <td>    1.078</td> <td> 0.282</td> <td>-1.45e+04</td> <td> 4.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_black</th>                     <td> 1.753e+04</td> <td> 1.62e+04</td> <td>    1.080</td> <td> 0.281</td> <td>-1.45e+04</td> <td> 4.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_hispanic</th>                  <td> 1.752e+04</td> <td> 1.62e+04</td> <td>    1.079</td> <td> 0.282</td> <td>-1.45e+04</td> <td> 4.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_asian</th>                     <td> 1.752e+04</td> <td> 1.62e+04</td> <td>    1.079</td> <td> 0.282</td> <td>-1.45e+04</td> <td> 4.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_aian</th>                      <td> 1.705e+04</td> <td> 1.62e+04</td> <td>    1.050</td> <td> 0.295</td> <td> -1.5e+04</td> <td> 4.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_nhpi</th>                      <td> 1.642e+04</td> <td> 1.62e+04</td> <td>    1.012</td> <td> 0.313</td> <td>-1.56e+04</td> <td> 4.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_twoormore</th>                 <td> 1.754e+04</td> <td> 1.62e+04</td> <td>    1.081</td> <td> 0.281</td> <td>-1.45e+04</td> <td> 4.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_nra</th>                       <td> 1.749e+04</td> <td> 1.62e+04</td> <td>    1.078</td> <td> 0.282</td> <td>-1.45e+04</td> <td> 4.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent_unknown</th>                   <td>  1.75e+04</td> <td> 1.62e+04</td> <td>    1.078</td> <td> 0.282</td> <td>-1.45e+04</td> <td> 4.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outstate_tuition</th>                  <td>   -0.0019</td> <td>    0.001</td> <td>   -2.091</td> <td> 0.038</td> <td>   -0.004</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tuition_revenue_per</th>               <td>   -0.0003</td> <td>    0.000</td> <td>   -0.585</td> <td> 0.559</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instructional_expenditure_per</th>     <td>-4.687e-05</td> <td>    0.000</td> <td>   -0.354</td> <td> 0.723</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_faculty_salary</th>                <td>   -0.0035</td> <td>    0.001</td> <td>   -3.117</td> <td> 0.002</td> <td>   -0.006</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ft_faculty_rate</th>                   <td>   -8.1358</td> <td>    8.448</td> <td>   -0.963</td> <td> 0.337</td> <td>  -24.796</td> <td>    8.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_net_price</th>                     <td>   -0.0006</td> <td>    0.000</td> <td>   -1.222</td> <td> 0.223</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>number_titleIV</th>                    <td>    0.0011</td> <td>    0.004</td> <td>    0.292</td> <td> 0.770</td> <td>   -0.006</td> <td>    0.008</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 6.911</td> <th>  Durbin-Watson:     </th> <td>   1.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.032</td> <th>  Jarque-Bera (JB):  </th> <td>  10.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.130</td> <th>  Prob(JB):          </th> <td> 0.00545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.010</td> <th>  Cond. No.          </th> <td>5.29e+09</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.29e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        rankingSortRank   R-squared:                       0.931\n",
       "Model:                            OLS   Adj. R-squared:                  0.919\n",
       "Method:                 Least Squares   F-statistic:                     76.97\n",
       "Date:                Tue, 14 Mar 2023   Prob (F-statistic):           3.83e-95\n",
       "Time:                        16:24:34   Log-Likelihood:                -982.14\n",
       "No. Observations:                 230   AIC:                             2034.\n",
       "Df Residuals:                     195   BIC:                             2155.\n",
       "Df Model:                          34                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================================\n",
       "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "Intercept                         -1.713e+04   1.62e+04     -1.056      0.292   -4.91e+04    1.49e+04\n",
       "act_avg                              -5.3288      1.760     -3.028      0.003      -8.799      -1.858\n",
       "admission_rate                      217.1199     68.479      3.171      0.002      82.064     352.175\n",
       "act_avg:admission_rate               -2.7438      2.669     -1.028      0.305      -8.008       2.520\n",
       "businessRepScore                      4.0317     10.578      0.381      0.704     -16.830      24.894\n",
       "businessRepScore:admission_rate     -42.8449     13.045     -3.285      0.001     -68.571     -17.118\n",
       "percent_parttime                    275.8040     63.612      4.336      0.000     150.349     401.259\n",
       "avg_cost                             -0.0019      0.001     -2.339      0.020      -0.004      -0.000\n",
       "percent_parttime:avg_cost            -0.0035      0.002     -2.140      0.034      -0.007      -0.000\n",
       "instante_tuition                      0.0026      0.001      2.747      0.007       0.001       0.004\n",
       "instante_tuition:businessRepScore  4.524e-05      0.000      0.242      0.809      -0.000       0.000\n",
       "sat_avg                              -0.0208      0.030     -0.701      0.484      -0.079       0.038\n",
       "percent_receiving_aid                -0.0003      0.145     -0.002      0.998      -0.286       0.285\n",
       "cost_after_aid                        0.0006      0.000      1.852      0.065   -3.78e-05       0.001\n",
       "hs_gpa_avg                          -11.5985      9.536     -1.216      0.225     -30.406       7.209\n",
       "tuition                               0.0007      0.001      0.796      0.427      -0.001       0.002\n",
       "engineeringRepScore                  -1.9759      3.550     -0.557      0.578      -8.978       5.026\n",
       "branches                             -0.6165      0.740     -0.833      0.406      -2.076       0.843\n",
       "ug_enrollment                        -0.0005      0.000     -1.418      0.158      -0.001       0.000\n",
       "percent_white                       1.75e+04   1.62e+04      1.078      0.282   -1.45e+04    4.95e+04\n",
       "percent_black                      1.753e+04   1.62e+04      1.080      0.281   -1.45e+04    4.95e+04\n",
       "percent_hispanic                   1.752e+04   1.62e+04      1.079      0.282   -1.45e+04    4.95e+04\n",
       "percent_asian                      1.752e+04   1.62e+04      1.079      0.282   -1.45e+04    4.95e+04\n",
       "percent_aian                       1.705e+04   1.62e+04      1.050      0.295    -1.5e+04    4.91e+04\n",
       "percent_nhpi                       1.642e+04   1.62e+04      1.012      0.313   -1.56e+04    4.84e+04\n",
       "percent_twoormore                  1.754e+04   1.62e+04      1.081      0.281   -1.45e+04    4.96e+04\n",
       "percent_nra                        1.749e+04   1.62e+04      1.078      0.282   -1.45e+04    4.95e+04\n",
       "percent_unknown                     1.75e+04   1.62e+04      1.078      0.282   -1.45e+04    4.95e+04\n",
       "outstate_tuition                     -0.0019      0.001     -2.091      0.038      -0.004      -0.000\n",
       "tuition_revenue_per                  -0.0003      0.000     -0.585      0.559      -0.001       0.001\n",
       "instructional_expenditure_per     -4.687e-05      0.000     -0.354      0.723      -0.000       0.000\n",
       "avg_faculty_salary                   -0.0035      0.001     -3.117      0.002      -0.006      -0.001\n",
       "ft_faculty_rate                      -8.1358      8.448     -0.963      0.337     -24.796       8.525\n",
       "avg_net_price                        -0.0006      0.000     -1.222      0.223      -0.001       0.000\n",
       "number_titleIV                        0.0011      0.004      0.292      0.770      -0.006       0.008\n",
       "==============================================================================\n",
       "Omnibus:                        6.911   Durbin-Watson:                   1.800\n",
       "Prob(Omnibus):                  0.032   Jarque-Bera (JB):               10.426\n",
       "Skew:                          -0.130   Prob(JB):                      0.00545\n",
       "Kurtosis:                       4.010   Cond. No.                     5.29e+09\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.29e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.ols('rankingSortRank~act_avg*admission_rate+businessRepScore*admission_rate+percent_parttime*avg_cost+instante_tuition*businessRepScore+' + '+'.join(predictors),data = df2).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)\n",
    "\n",
    "You may or may not have code to put in this section. Delete this section if it is irrelevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
